{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2(Generative).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11ShTmMw2INKM8NrKcSEMjkc8Q22nzFYg",
      "authorship_tag": "ABX9TyNufE5oSEsOJS1nqXymTWkH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ICPCfirst/DeepLearningHW2/blob/main/HW2(Generative).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "X_train_fpath = \"/content/drive/MyDrive/data/X_train\"\n",
        "X_test_fpath = \"/content/drive/MyDrive/data/X_test\"\n",
        "Y_train_fpath = \"/content/drive/MyDrive/data/Y_train\"\n",
        "output_fpath = \"./output_{}.csv\""
      ],
      "metadata": {
        "id": "ZtqTY2uoYU_5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(X_train_fpath) as f:\n",
        "  next(f)\n",
        "  X_train = np.array([line.strip(\"\\n\").split(\",\")[1:] for line in f],dtype = float)\n",
        "with open(X_test_fpath) as f:\n",
        "  next(f)\n",
        "  X_test = np.array([line.strip(\"\\n\").split(\",\")[1:] for line in f],dtype = float)\n",
        "with open(Y_train_fpath) as f:\n",
        "  next(f)\n",
        "  Y_train = np.array([line.strip(\"\\n\").split(\",\")[1:] for line in f],dtype = float)\n",
        "  "
      ],
      "metadata": {
        "id": "DMYGW_fhZHkX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _normalize(X,train = True,specified_column = None,X_mean = None,X_std = None):\n",
        "  if specified_column == None:\n",
        "    specified_column = np.arange(X.shape[1])\n",
        "  if train:\n",
        "    X_mean = np.mean(X[:,specified_column],0).reshape(1,-1) # meean(0)==压缩行 ；reshape == 必须转化为一行n列 ？？？？？？？是不是重复了\n",
        "    X_std = np.std(X[:,specified_column]).reshape(1,-1)\n",
        "  \n",
        "  X[:,specified_column] = (X[:,specified_column]-X_mean)/(X_std + 1e-8)# 进行归一化，中心化和标准处理化\n",
        "  return X,X_mean,X_std"
      ],
      "metadata": {
        "id": "TY0kR05ZcCVR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _train_dev_split(X,Y,dev_ratio = 0.25):\n",
        "  train_size = int(len(X)*(1-dev_ratio))\n",
        "  return X[:train_size],Y[:train_size],X[train_size:],Y[train_size:]"
      ],
      "metadata": {
        "id": "1rnFR-P2fDH9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_mean,X_std = _normalize(X_train,train = True)\n",
        "X_test,_,_ = _normalize(X_test,train = False,specified_column=None,X_mean = X_mean,X_std = X_std)\n",
        "\n",
        "dev_ratio = 0.1\n",
        "X_train,Y_train,X_dev,Y_dev = _train_dev_split(X_train,Y_train,dev_ratio = dev_ratio) #自己根据已给数据分开来使程序准确性更高\n",
        "\n",
        "train_size = X_train.shape[0]\n",
        "dev_size = X_dev.shape[0]\n",
        "test_size = X_test.shape[0]\n",
        "data_dim = X_train.shape[1]"
      ],
      "metadata": {
        "id": "ywFAsLGA_SGb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _shuffle(X, Y):  #洗牌，排序\n",
        "    # 此函数将两个等长的列表/数组 X 和 Y 混在一起。\n",
        "    randomize = np.arange(len(X))\n",
        "    np.random.shuffle(randomize)\n",
        "    return (X[randomize], Y[randomize])    #作用是将行重新打乱。"
      ],
      "metadata": {
        "id": "DSGbtVaaCdU1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _sigmoid(z):            #经典\n",
        "    # Sigmoid function can be used to calculate probability.\n",
        "    # To avoid overflow, minimum/maximum output value is set.\n",
        "    return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))#np.clip(a,min,max),作用是将所有输出的a都限制在min和max之间。"
      ],
      "metadata": {
        "id": "LYt0p-m6FtYD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _f(X, w, b):\n",
        "     # 这是逻辑回归函数，由 w 和 b 参数化\n",
        "     #\n",
        "     # 论点：\n",
        "     # X: 输入数据，shape = [batch_size, data_dimension] # 【8，510】\n",
        "     # w: 权重向量, shape = [data_dimension, ] # 【510，】\n",
        "     # b: 偏差，标量\n",
        "     # 输出：\n",
        "     # X 的每一行被正标记的预测概率，shape = [batch_size, ]\n",
        "    return _sigmoid(np.matmul(X, w) + b)#二维乘法。  返回n行1列"
      ],
      "metadata": {
        "id": "owGoLoADGdeF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _predict(X, w, b):\n",
        "    # 此函数返回 X 的每一行的真值预测+\n",
        "    # 通过四舍五入逻辑回归函数的结果。\n",
        "    return np.round(_f(X, w, b)).astype(np.int)# 这里是取整 取整后容易产生梯度爆炸或溢出，故之后的loss不采用此函数；全精度>混合精度>半精度"
      ],
      "metadata": {
        "id": "6oSqzGsEGfUn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _accuracy(Y_pred, Y_label):# 精度越大越好\n",
        "    # 这个函数计算预测精度\n",
        "    acc = 1 - np.mean(np.abs(Y_pred - Y_label))\n",
        "    return acc   "
      ],
      "metadata": {
        "id": "L3IZwvzdGhM3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _cross_entropy_loss(y_pred, Y_label):\n",
        "    224 / 5,000\n",
        "    # 这个函数计算交叉熵。\n",
        "    #\n",
        "    # 论点：\n",
        "    # y_pred：概率预测，浮点向量\n",
        "    # Y_label：ground truth 标签，布尔向量\n",
        "    # 输出：\n",
        "    # 交叉熵，标量\n",
        "    cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred))# np.dot 矩阵乘积\n",
        "    return cross_entropy"
      ],
      "metadata": {
        "id": "4_y-B4tfGi1L"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _gradient(X, Y_label, w, b):\n",
        "    # 此函数计算交叉熵损失相对于权重 w 和偏差 b 的梯度。\n",
        "    y_pred = _f(X, w, b)\n",
        "    pred_error = Y_label - y_pred\n",
        "    w_grad = -np.sum(pred_error * X.T, 1)  # -E(y_ - y)*X.T T(转置)\n",
        "    b_grad = -np.sum(pred_error)  #可不可以换成mean\n",
        "    return w_grad, b_grad\n"
      ],
      "metadata": {
        "id": "rqAhYsZyGlUu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 分别计算类别0和类别1的均值\n",
        "X_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])  # u0\n",
        "X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])  # u1\n",
        "\n",
        "mean_0 = np.mean(X_train_0, axis = 0)\n",
        "mean_1 = np.mean(X_train_1, axis = 0)\n",
        "\n",
        "#print(X_train_0.shape) #(38784, 510) 有三万多行数据的结果是u0\n",
        "#print(\"*\"*50) \n",
        "#print(X_train_1.shape) #(10046, 510)) 有一万多行数据的结果是u1\n",
        "#print(\"*\"*50)\n",
        "#print(mean_0.shape)\n",
        "#print(\"*\"*50)\n",
        "#print(mean_1.shape)"
      ],
      "metadata": {
        "id": "wwIAZv3WGnEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 分别计算类别0和类别1的协方差\n",
        "cov_0 = np.zeros((data_dim, data_dim))\n",
        "cov_1 = np.zeros((data_dim, data_dim))\n",
        "\n",
        "for x in X_train_0:\n",
        "    cov_0 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]\n",
        "for x in X_train_1:\n",
        "    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]\n",
        "\n",
        "# 共享协方差 = 独立的协方差的加权求和\n",
        "cov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train_0.shape[0] + X_train_1.shape[0])\n"
      ],
      "metadata": {
        "id": "vhzNgaUJAXoT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}